{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(F\"Devices: {physical_devices}\")\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import optuna\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, \\\n",
    "                        Dropout, Average, Maximum, Dot, Reshape, TimeDistributed,\\\n",
    "                        Flatten, Conv1D, MaxPooling1D, AveragePooling1D, \\\n",
    "                        LeakyReLU, LSTM, Lambda,UpSampling1D,BatchNormalization, AdditiveAttention, Attention\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "import pipelineLibrary\n",
    "import utilsCode\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 1\n",
    "BATCHSIZE = 1024\n",
    "\n",
    "n_studies = 3\n",
    "n_trials = 10\n",
    "n_splits = 5\n",
    "\n",
    "# Config\n",
    "output_filename = \"../../../ML-dashboard.html\"\n",
    "input_filename = '../../../dados_anon_fulldataset.csv'\n",
    "checkpoint_filepath = '/tmp/checkpoint-ML'\n",
    "\n",
    "validation_freq = 10\n",
    "monitor_evaluation = \"val_acc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilsCode.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_filename)\n",
    "\n",
    "df = df.drop(columns=['Unnamed: 0'])\\\n",
    "        .drop_duplicates()\\\n",
    "        .copy()\n",
    "\n",
    "new_state = True\n",
    "alunos_list = df['ID_CURSO_ALUNO'].unique()\n",
    "if (new_state):\n",
    "    train_validation_perc = 0.7\n",
    "    alunos_train_validation = np.random.choice(len(alunos_list), size=int(len(alunos_list)*train_validation_perc), replace=False)\n",
    "    np.save('random_state_.npy', alunos_train_validation)\n",
    "else:\n",
    "    alunos_train_validation = np.load('random_state_.npy')\n",
    "data_train_validation_ = df[df['ID_CURSO_ALUNO'].isin(alunos_list[alunos_train_validation])]\n",
    "data_test_ = df[~df['ID_CURSO_ALUNO'].isin(alunos_list[alunos_train_validation])]\n",
    "\n",
    "alunos_list_train_validation = data_train_validation_['ID_CURSO_ALUNO'].unique()\n",
    "alunos_list_test = data_test_['ID_CURSO_ALUNO'].unique()\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAUC(predicted, label):\n",
    "    auc = metrics.roc_auc_score(label, predicted)    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customRecall(predicted, label):\n",
    "    tn, fp, fn, tp = confusion_matrix(predicted>=threshold, label).ravel()\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def customAccuracy(predicted, label):\n",
    "    return sum((predicted>=threshold)==label)/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_default_lstm(parameters, np_estatico, np_series_lstm):\n",
    "    _, ncolumns_static = np_estatico.shape\n",
    "    _, timestep_lstm, width_lstm = np_series_lstm.shape\n",
    "    \n",
    "    static_input = Input(name='static_input', shape=(ncolumns_static,))\n",
    "    series_input = Input(name='series_input', shape=(timestep_lstm, width_lstm,))\n",
    "\n",
    "    lstm = LSTM(parameters['lstm_output'],\n",
    "                dropout=parameters['lstm_dropout'],\n",
    "                recurrent_dropout=parameters['recurrent_dropout'],\n",
    "                return_sequences=True,\n",
    "                name='lstm_layer')(series_input)\n",
    "    \n",
    "    flat = Flatten()(lstm)\n",
    "    \n",
    "    \n",
    "    dense_static = Dense(parameters['dense_1_static'], activation=\"relu\", name='dense_static')(static_input)\n",
    "    bnorm = BatchNormalization()(dense_static)\n",
    "    drop = Dropout(parameters['dense_dropout_1'])(bnorm)\n",
    "    dense_static2 = Dense(parameters['dense_2_static'], activation=\"relu\", name='dense_static2')(drop)\n",
    "    bnorm2 = BatchNormalization()(dense_static2)\n",
    "    drop2 = Dropout(parameters['dense_dropout_2'])(bnorm2)\n",
    "    \n",
    "    concanate_layer = Concatenate(name='concanate_layer')([flat, drop2])\n",
    "\n",
    "    dense1 = Dense(parameters['merged_dense_1'], activation='relu', name='dense1')(concanate_layer)\n",
    "    bnorm3 = BatchNormalization()(dense1)\n",
    "    drop3 = Dropout(parameters['merged_dense_dropout_1'])(bnorm3)\n",
    "    dense2 = Dense(parameters['merged_dense_2'], activation='relu', name='dense2')(drop3)\n",
    "    bnorm4 = BatchNormalization()(dense2)\n",
    "    drop4 = Dropout(parameters['merged_dense_dropout_2'])(bnorm4)\n",
    "    output = Dense(1, activation='sigmoid', name='output_layer')(drop4)\n",
    "    \n",
    "    optimizer = optimizers.Adadelta(learning_rate=parameters['learning_rate'], rho=parameters['rho'])\n",
    "\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[static_input, series_input] , outputs=output)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=optimizer, metrics=[\"acc\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LSTM ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION\n",
    "def objective_lstm(trial):\n",
    "    parameters = {'learning_rate': trial.suggest_loguniform('learning_rate', 0.00095, 0.005),\n",
    "                    'rho': trial.suggest_uniform('rho', 0.9, 1),\n",
    "                    'epoach': trial.suggest_int('epoach', 250, 750),\n",
    "\n",
    "                    'dropout': trial.suggest_uniform('dropout', 0, 0.5),\n",
    "                    'recurrent_dropout': trial.suggest_uniform('recurrent_dropout', 0, 0.5),\n",
    "\n",
    "                    'lstm_output': trial.suggest_int('lstm_output', 100, 500),\n",
    "\n",
    "                    'dense_static': trial.suggest_int('dense_static', 100, 500),\n",
    "\n",
    "                    'merged_dense': trial.suggest_int('merged_dense', 100, 500),\n",
    "                  }\n",
    "\n",
    "    parameters_lstm = {'learning_rate': parameters['learning_rate'],\n",
    "                    'rho': parameters['rho'],\n",
    "                    'epoach': parameters['epoach'],\n",
    "                                              \n",
    "                    'lstm_output': parameters['lstm_output'],\n",
    "                    'lstm_dropout': parameters['dropout'],\n",
    "                    'recurrent_dropout': parameters['recurrent_dropout'],\n",
    "                        \n",
    "                    'dense_1_static': parameters['dense_static'],\n",
    "                    'dense_dropout_1': parameters['dropout'],\n",
    "                    'dense_2_static': parameters['dense_static'],\n",
    "                    'dense_dropout_2': parameters['dropout'],\n",
    "                      \n",
    "                    'merged_dense_1': parameters['merged_dense'],\n",
    "                    'merged_dense_dropout_1': parameters['dropout'],\n",
    "                    'merged_dense_2': parameters['merged_dense'],\n",
    "                    'merged_dense_dropout_2': parameters['dropout'],\n",
    "                    }\n",
    "        \n",
    "    acc_list = []\n",
    "    for train_index, test_index in kf.split(alunos_list_train_validation):\n",
    "        data_train = df[df['ID_CURSO_ALUNO'].isin(alunos_list_train_validation[train_index])]\n",
    "        time_series_dataframe_train, np_estatico_train, label_train = pipelineLibrary.pipe_default.fit_transform(data_train)\n",
    "        np_series_lstm_train = pipelineLibrary.pipe_lstm.fit_transform(time_series_dataframe_train)\n",
    "        \n",
    "        data_validation = df[df['ID_CURSO_ALUNO'].isin(alunos_list_train_validation[test_index])]\n",
    "        time_series_dataframe_validation, np_estatico_validation, label_validation = pipelineLibrary.pipe_default.transform(data_validation)\n",
    "        np_series_lstm_validation = pipelineLibrary.pipe_lstm.transform(time_series_dataframe_validation)\n",
    "\n",
    "        model = model_default_lstm(parameters_lstm, np_estatico_train, np_series_lstm_train)\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit([np_estatico_train, np_series_lstm_train],\n",
    "              label_train,\n",
    "              epochs=parameters_lstm['epoach'],\n",
    "              verbose=0,\n",
    "              batch_size=BATCHSIZE,\n",
    "                )\n",
    "        \n",
    "#         # Evaluate\n",
    "        predicted = model.predict([np_estatico_validation, np_series_lstm_validation])\n",
    "        acc_list.append(customAccuracy(predicted, label_validation))\n",
    "\n",
    "        clear_session()\n",
    "        del model, time_series_dataframe_train, np_series_lstm_train, label_validation, np_series_lstm_validation, \\\n",
    "            np_estatico_train, label_train, time_series_dataframe_validation, np_estatico_validation\n",
    "            \n",
    "        gc.collect()\n",
    "\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "    # EVALUATION\n",
    "    accuracyList = []\n",
    "    sensibilidadeList = []\n",
    "    especificidadeList = []\n",
    "    aucList = []\n",
    "\n",
    "    parameters = study.best_params\n",
    "    parameters_lstm = {'learning_rate': parameters['learning_rate'],\n",
    "                    'rho': parameters['rho'],\n",
    "                    'epoach': parameters['epoach'],\n",
    "\n",
    "                    'lstm_output': parameters['lstm_output'],\n",
    "                    'lstm_dropout': parameters['dropout'],\n",
    "                    'recurrent_dropout': parameters['recurrent_dropout'],\n",
    "\n",
    "                    'dense_1_static': parameters['dense_static'],\n",
    "                    'dense_dropout_1': parameters['dropout'],\n",
    "                    'dense_2_static': parameters['dense_static'],\n",
    "                    'dense_dropout_2': parameters['dropout'],\n",
    "\n",
    "                    'merged_dense_1': parameters['merged_dense'],\n",
    "                    'merged_dense_dropout_1': parameters['dropout'],\n",
    "                    'merged_dense_2': parameters['merged_dense'],\n",
    "                    'merged_dense_dropout_2': parameters['dropout'],\n",
    "                    }\n",
    "\n",
    "    for train_index, test_index in kf.split(alunos_list_test):\n",
    "        # Train model using all training data\n",
    "        data_train = df[df['ID_CURSO_ALUNO'].isin(alunos_list_train_validation)]\n",
    "        time_series_dataframe_train, np_estatico_train, label_train = pipelineLibrary.pipe_default.fit_transform(data_train)\n",
    "        np_series_lstm_train = pipelineLibrary.pipe_lstm.fit_transform(time_series_dataframe_train)\n",
    "\n",
    "        model = model_default_lstm(parameters_lstm, np_estatico_train, np_series_lstm_train)\n",
    "\n",
    "        # Save best model\n",
    "        monitor = monitor_evaluation\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor=monitor,\n",
    "            save_best_only=True)\n",
    "        \n",
    "        callbacks = [model_checkpoint_callback]\n",
    "        \n",
    "        # Train\n",
    "        model.fit([np_estatico_train, np_series_lstm_train],\n",
    "              label_train,\n",
    "              epochs=study.best_params['epoach'],\n",
    "              verbose=0,\n",
    "              batch_size=BATCHSIZE,\n",
    "              validation_split=0.3,\n",
    "              callbacks=callbacks,\n",
    "                 )    \n",
    "        del time_series_dataframe_train, np_series_lstm_train, np_estatico_train, label_train\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate\n",
    "        # Evaluate using k-fold test dataset partition\n",
    "        data_test = df[df['ID_CURSO_ALUNO'].isin(alunos_list_test[test_index])]\n",
    "        time_series_dataframe_test, np_estatico_test, label_test = pipelineLibrary.pipe_default.transform(data_test)\n",
    "        np_series_lstm_test = pipelineLibrary.pipe_lstm.transform(time_series_dataframe_test) \n",
    "\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        predicted = model.predict([np_estatico_test, np_series_lstm_test])\n",
    "        tn, fp, fn, tp = confusion_matrix(predicted>=threshold, label_test).ravel()\n",
    "\n",
    "        unique, counts = np.unique(label_test, return_counts=True)\n",
    "        print(F\"Labels: {dict(zip(unique, counts))}\")\n",
    "        unique, counts = np.unique((predicted>=threshold).astype(int), return_counts=True)\n",
    "        print(F\"Predicted: {dict(zip(unique, counts))}\")\n",
    "\n",
    "        sensibilidadeList.append(tp/(tp+fn))\n",
    "        especificidadeList.append(tn/(tn+fp))\n",
    "        aucList.append(getAUC(predicted, label_test))\n",
    "        accuracyList.append(customAccuracy(predicted, label_test))\n",
    "\n",
    "        clear_session()\n",
    "        del model, label_test, np_series_lstm_test, time_series_dataframe_test, np_estatico_test\n",
    "        gc.collect()\n",
    "\n",
    "    print(accuracyList)\n",
    "    print(sensibilidadeList)\n",
    "    print(especificidadeList)\n",
    "    print(aucList)\n",
    "    return np.array(accuracyList).mean(), np.array(sensibilidadeList).mean(), np.array(especificidadeList).mean(), np.array(aucList).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2021-01-08 23:19:37,347]\u001B[0m A new study created in memory with name: no-name-91a219e3-67ad-4b74-909a-c7a470809350\u001B[0m\n",
      "\u001B[32m[I 2021-01-08 23:20:14,112]\u001B[0m Trial 0 finished with value: 0.6954427659511566 and parameters: {'learning_rate': 0.001627121194946629, 'rho': 0.9271964095852862, 'epoach': 647, 'lstm_output': 361, 'lstm_dropout': 0.3420757846157449, 'recurrent_dropout': 0.3421411267904899, 'dense_1_static': 126, 'dense_dropout_1': 0.40624974973504163, 'dense_2_static': 103, 'dense_dropout_2': 0.24358400929889257, 'merged_dense_1': 370, 'merged_dense_dropout_1': 0.12183746330839668, 'merged_dense_2': 320, 'merged_dense_dropout_2': 0.0586619878364536}. Best is trial 0 with value: 0.6954427659511566.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean runned epochs: 66.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2021-01-08 23:21:59,546]\u001B[0m Trial 1 finished with value: 0.5181376188993454 and parameters: {'learning_rate': 0.0027010860958801113, 'rho': 0.9840728368519109, 'epoach': 401, 'lstm_output': 432, 'lstm_dropout': 0.20564589244172649, 'recurrent_dropout': 0.44917416363800716, 'dense_1_static': 207, 'dense_dropout_1': 0.2798976389387046, 'dense_2_static': 101, 'dense_dropout_2': 0.1960181575425382, 'merged_dense_1': 121, 'merged_dense_dropout_1': 0.05408993704747306, 'merged_dense_2': 174, 'merged_dense_dropout_2': 0.1006139867554337}. Best is trial 0 with value: 0.6954427659511566.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean runned epochs: 206.0\n",
      "Study: 0\n",
      "Best Value: 0.6954427659511566\n",
      "Best Parameters: {'learning_rate': 0.001627121194946629, 'rho': 0.9271964095852862, 'epoach': 647, 'lstm_output': 361, 'lstm_dropout': 0.3420757846157449, 'recurrent_dropout': 0.3421411267904899, 'dense_1_static': 126, 'dense_dropout_1': 0.40624974973504163, 'dense_2_static': 103, 'dense_dropout_2': 0.24358400929889257, 'merged_dense_1': 370, 'merged_dense_dropout_1': 0.12183746330839668, 'merged_dense_2': 320, 'merged_dense_dropout_2': 0.0586619878364536}\n",
      "(7, 11, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-2a6244614df8>:54: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in long_scalars\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "sens = []\n",
    "esp = []\n",
    "auc = []\n",
    "\n",
    "figs = []\n",
    "for i in range(n_studies):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "\n",
    "    study.optimize(objective_lstm, n_trials=n_trials)\n",
    "    \n",
    "    print(F\"Study: {i}\")\n",
    "    print(F\"Best Value: {study.best_value}\")\n",
    "    print(F\"Best Parameters: {study.best_params}\")\n",
    "    \n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    figs.append(fig)\n",
    "    \n",
    "    a, b, c, d = evaluation()\n",
    "    acc.append(a)\n",
    "    sens.append(b)\n",
    "    esp.append(c)\n",
    "    auc.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilsCode.figures_to_html(figs, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>              </td><td>média </td><td>desvio padrão</td></tr>\n",
       "<tr><td>Acurácia      </td><td>0.4313</td><td>0.0          </td></tr>\n",
       "<tr><td>Sensibilidade </td><td>nan   </td><td>nan          </td></tr>\n",
       "<tr><td>Especificidade</td><td>0.5278</td><td>0.0          </td></tr>\n",
       "<tr><td>AUC           </td><td>0.3483</td><td>0.0          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [\n",
    "    ['', 'média', 'desvio padrão'],\n",
    "    ['Acurácia', round(np.array(acc).mean(),4),  round(np.array(acc).std(),4)],\n",
    "    ['Sensibilidade', round(np.array(sens).mean(),4),  round(np.array(sens).std(),4)],\n",
    "    ['Especificidade', round(np.array(esp).mean(),4),  round(np.array(esp).std(),4)],\n",
    "    ['AUC', round(np.array(auc).mean(),4),  round(np.array(auc).std(),4)]\n",
    "]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43128654970760233]\n",
      "[nan]\n",
      "[0.5277777777777778]\n",
      "[0.3483183483183483]\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(sens)\n",
    "print(esp)\n",
    "print(auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}