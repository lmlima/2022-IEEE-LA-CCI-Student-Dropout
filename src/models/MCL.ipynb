{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(F\"Devices: {physical_devices}\")\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import optuna\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, \\\n",
    "                        Dropout, Average, Maximum, Dot, Reshape, TimeDistributed,\\\n",
    "                        Flatten, Conv1D, MaxPooling1D, AveragePooling1D, \\\n",
    "                        LeakyReLU, LSTM, Lambda,UpSampling1D,BatchNormalization, AdditiveAttention, Attention\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "import pipelineLibrary\n",
    "import utilsCode\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 1\n",
    "BATCHSIZE = 1024\n",
    "\n",
    "n_studies = 3\n",
    "n_trials = 10\n",
    "n_splits = 5\n",
    "\n",
    "# Config\n",
    "output_filename = \"../../../MCL-dashboard.html\"\n",
    "input_filename = '../../../dados_anon_fulldataset.csv'\n",
    "checkpoint_filepath = '/tmp/checkpoint-MCL'\n",
    "\n",
    "validation_freq = 10\n",
    "monitor_evaluation = \"val_acc\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utilsCode.set_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_filename)\n",
    "\n",
    "df = df.drop(columns=['Unnamed: 0'])\\\n",
    "        .drop_duplicates()\\\n",
    "        .copy()\n",
    "\n",
    "new_state = True\n",
    "alunos_list = df['ID_CURSO_ALUNO'].unique()\n",
    "if (new_state):\n",
    "    train_validation_perc = 0.7\n",
    "    alunos_train_validation = np.random.choice(len(alunos_list), size=int(len(alunos_list)*train_validation_perc), replace=False)\n",
    "    np.save('random_state_.npy', alunos_train_validation)\n",
    "else:\n",
    "    alunos_train_validation = np.load('random_state_.npy')\n",
    "data_train_validation_ = df[df['ID_CURSO_ALUNO'].isin(alunos_list[alunos_train_validation])]\n",
    "data_test_ = df[~df['ID_CURSO_ALUNO'].isin(alunos_list[alunos_train_validation])]\n",
    "\n",
    "alunos_list_train_validation = data_train_validation_['ID_CURSO_ALUNO'].unique()\n",
    "alunos_list_test = data_test_['ID_CURSO_ALUNO'].unique()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAUC(predicted, label):\n",
    "    auc = metrics.roc_auc_score(label, predicted)    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customRecall(predicted, label):\n",
    "    tn, fp, fn, tp = confusion_matrix(predicted>=threshold, label).ravel()\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def customAccuracy(predicted, label):\n",
    "    return sum((predicted>=threshold)==label)/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn(parameters, np_estatico, np_series_cnn):\n",
    "    _, ncolumns_static = np_estatico.shape\n",
    "    _, timestep_cnn, width_cnn, height_cnn = np_series_cnn.shape\n",
    "    \n",
    "    static_input = Input(name='static_input', shape=(ncolumns_static,))\n",
    "    series_input = Input(name='series_input', shape=(timestep_cnn, width_cnn, height_cnn))\n",
    "    \n",
    "    conv_layer_1 = TimeDistributed(Conv1D(filters=10, kernel_size=5), name='conv_layer_1')(series_input)\n",
    "    maxpooling_layer_1 = TimeDistributed(MaxPooling1D(3), name='maxpooling_layer_1')(conv_layer_1)\n",
    "    drop_cnn_1 = Dropout(parameters['cnn_dropout_1'])(maxpooling_layer_1)\n",
    "    reshape_layer_attention1 = Reshape((4, 100))(drop_cnn_1)\n",
    "    \n",
    "    lstm = LSTM(parameters['lstm_output'], \n",
    "                dropout=parameters['lstm_dropout'], \n",
    "                recurrent_dropout=parameters['recurrent_dropout'], \n",
    "                return_sequences=True,\n",
    "                name='lstm_layer')(reshape_layer_attention1)\n",
    "        \n",
    "    flat = Flatten()(lstm)\n",
    "    \n",
    "    dense_static = Dense(parameters['dense_1_static'], activation=\"relu\", name='dense_static')(static_input)\n",
    "    bnorm = BatchNormalization()(dense_static)\n",
    "    drop = Dropout(parameters['dense_dropout_1'])(bnorm)\n",
    "    dense_static2 = Dense(parameters['dense_2_static'], activation=\"relu\", name='dense_static2')(drop)\n",
    "    bnorm2 = BatchNormalization()(dense_static2)\n",
    "    drop2 = Dropout(parameters['dense_dropout_2'])(bnorm2)\n",
    "    \n",
    "    concanate_layer = Concatenate(name='concanate_layer')([flat, drop2])\n",
    "\n",
    "    dense1 = Dense(parameters['merged_dense_1'], activation='relu', name='dense1')(concanate_layer)\n",
    "    bnorm3 = BatchNormalization()(dense1)\n",
    "    drop3 = Dropout(parameters['merged_dense_dropout_1'])(bnorm3)\n",
    "    dense2 = Dense(parameters['merged_dense_2'], activation='relu', name='dense2')(drop3)\n",
    "    bnorm4 = BatchNormalization()(dense2)\n",
    "    drop4 = Dropout(parameters['merged_dense_dropout_2'])(bnorm4)\n",
    "    output = Dense(1, activation='sigmoid', name='output_layer')(drop4)\n",
    "    \n",
    "    optimizer = optimizers.Adadelta(learning_rate=parameters['learning_rate'], rho=parameters['rho'])\n",
    "\n",
    "    model = Model(inputs=[static_input, series_input] , outputs=output)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=optimizer, metrics=[\"acc\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CNN ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_cnn(trial):\n",
    "    parameters = {'learning_rate': trial.suggest_loguniform('learning_rate', 0.00095, 0.005),\n",
    "                    'rho': trial.suggest_uniform('rho', 0.9, 1),\n",
    "                    'epoach': trial.suggest_int('epoach', 250, 750),\n",
    "\n",
    "                    'dropout': trial.suggest_uniform('dropout', 0, 0.5),\n",
    "                    'recurrent_dropout': trial.suggest_uniform('recurrent_dropout', 0, 0.5),\n",
    "\n",
    "                    'lstm_output': trial.suggest_int('lstm_output', 100, 500),\n",
    "\n",
    "                    'dense_static': trial.suggest_int('dense_static', 100, 500),\n",
    "\n",
    "                    'merged_dense': trial.suggest_int('merged_dense', 100, 500),\n",
    "                  }\n",
    "\n",
    "    parameters_cnn = {'learning_rate': parameters['learning_rate'],\n",
    "                    'rho': parameters['rho'],\n",
    "                    'epoach': parameters['epoach'],\n",
    "\n",
    "                    'cnn_dropout_1': parameters['dropout'],\n",
    "\n",
    "                    'lstm_output': parameters['lstm_output'],\n",
    "                    'lstm_dropout': parameters['dropout'],\n",
    "                    'recurrent_dropout': parameters['recurrent_dropout'],\n",
    "\n",
    "                    'dense_1_static': parameters['dense_static'],\n",
    "                    'dense_dropout_1': parameters['dropout'],\n",
    "                    'dense_2_static': parameters['dense_static'],\n",
    "                    'dense_dropout_2': parameters['dropout'],\n",
    "\n",
    "                    'merged_dense_1': parameters['merged_dense'],\n",
    "                    'merged_dense_dropout_1': parameters['dropout'],\n",
    "                    'merged_dense_2': parameters['merged_dense'],\n",
    "                    'merged_dense_dropout_2': parameters['dropout'],\n",
    "                    }\n",
    "\n",
    "    acc_list = []\n",
    "    for train_index, test_index in kf.split(alunos_list_train_validation):\n",
    "        data_train = df[df['ID_CURSO_ALUNO'].isin(alunos_list_train_validation[train_index])]\n",
    "        time_series_dataframe_train, np_estatico_train, label_train = pipelineLibrary.pipe_default.fit_transform(data_train)\n",
    "        np_series_cnn_train = pipelineLibrary.pipe_cnn.fit_transform(time_series_dataframe_train)\n",
    "\n",
    "        data_validation = df[df['ID_CURSO_ALUNO'].isin(alunos_list_train_validation[test_index])]\n",
    "        time_series_dataframe_validation, np_estatico_validation, label_validation = pipelineLibrary.pipe_default.transform(data_validation)\n",
    "        np_series_cnn_validation = pipelineLibrary.pipe_cnn.transform(time_series_dataframe_validation)\n",
    "\n",
    "        gc.collect()\n",
    "        model = model_cnn(parameters_cnn, np_estatico_train, np_series_cnn_train)\n",
    "\n",
    "        # Train\n",
    "        history = model.fit([np_estatico_train, np_series_cnn_train],\n",
    "              label_train,\n",
    "              epochs=parameters_cnn['epoach'],\n",
    "              verbose=0,\n",
    "              batch_size=BATCHSIZE,\n",
    "                )\n",
    "        \n",
    "        del time_series_dataframe_train, np_estatico_train, label_train, np_series_cnn_train, data_train\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate\n",
    "        predicted = model.predict([np_estatico_validation, np_series_cnn_validation])\n",
    "        acc_list.append(customAccuracy(predicted, label_validation))\n",
    "        \n",
    "        clear_session()\n",
    "        del model, time_series_dataframe_validation, np_estatico_validation, label_validation, np_series_cnn_validation, data_validation\n",
    "        gc.collect()         \n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "    # EVALUATION\n",
    "    accuracyList = []\n",
    "    sensibilidadeList = []\n",
    "    especificidadeList = []\n",
    "    aucList = []\n",
    "\n",
    "    parameters = study.best_params\n",
    "    parameters_cnn = {'learning_rate': parameters['learning_rate'],\n",
    "                    'rho': parameters['rho'],\n",
    "                    'epoach': parameters['epoach'],\n",
    "\n",
    "                    'cnn_dropout_1': parameters['dropout'],\n",
    "\n",
    "                    'lstm_output': parameters['lstm_output'],\n",
    "                    'lstm_dropout': parameters['dropout'],\n",
    "                    'recurrent_dropout': parameters['recurrent_dropout'],\n",
    "\n",
    "                    'dense_1_static': parameters['dense_static'],\n",
    "                    'dense_dropout_1': parameters['dropout'],\n",
    "                    'dense_2_static': parameters['dense_static'],\n",
    "                    'dense_dropout_2': parameters['dropout'],\n",
    "\n",
    "                    'merged_dense_1': parameters['merged_dense'],\n",
    "                    'merged_dense_dropout_1': parameters['dropout'],\n",
    "                    'merged_dense_2': parameters['merged_dense'],\n",
    "                    'merged_dense_dropout_2': parameters['dropout'],\n",
    "                    }\n",
    "\n",
    "    for train_index, test_index in kf.split(alunos_list_test):\n",
    "        # Train model using all training data\n",
    "        data_train = df[df['ID_CURSO_ALUNO'].isin(alunos_list_train_validation)]\n",
    "        time_series_dataframe_train, np_estatico_train, label_train = pipelineLibrary.pipe_default.fit_transform(data_train)\n",
    "        np_series_cnn_train = pipelineLibrary.pipe_cnn.fit_transform(time_series_dataframe_train)\n",
    "\n",
    "        model = model_cnn(parameters_cnn, np_estatico_train, np_series_cnn_train)\n",
    "\n",
    "        # Save best model\n",
    "        monitor = monitor_evaluation\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor=monitor,\n",
    "            save_best_only=True)\n",
    "\n",
    "        callbacks = [model_checkpoint_callback]\n",
    "\n",
    "        # Train\n",
    "        model.fit([np_estatico_train, np_series_cnn_train],\n",
    "              label_train,\n",
    "              epochs=study.best_params['epoach'],\n",
    "              verbose=0,\n",
    "              batch_size=BATCHSIZE,\n",
    "              validation_split=0.3,\n",
    "              callbacks=callbacks,\n",
    "                 )\n",
    "        del time_series_dataframe_train, np_estatico_train, label_train, np_series_cnn_train\n",
    "        gc.collect()\n",
    "\n",
    "        # Evaluate\n",
    "        # Evaluate using k-fold test dataset partition\n",
    "        data_test = df[df['ID_CURSO_ALUNO'].isin(alunos_list_test[test_index])]\n",
    "        time_series_dataframe_test, np_estatico_test, label_test = pipelineLibrary.pipe_default.transform(data_test)\n",
    "        np_series_cnn_test = pipelineLibrary.pipe_cnn.transform(time_series_dataframe_test)\n",
    "\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        predicted = model.predict([np_estatico_test, np_series_cnn_test])\n",
    "        tn, fp, fn, tp = confusion_matrix(predicted>=threshold, label_test).ravel()\n",
    "\n",
    "        unique, counts = np.unique(label_test, return_counts=True)\n",
    "        print(F\"Labels: {dict(zip(unique, counts))}\")\n",
    "        unique, counts = np.unique((predicted>=threshold).astype(int), return_counts=True)\n",
    "        print(F\"Predicted: {dict(zip(unique, counts))}\")\n",
    "\n",
    "\n",
    "        sensibilidadeList.append(tp/(tp+fn))\n",
    "        especificidadeList.append(tn/(tn+fp))\n",
    "        aucList.append(getAUC(predicted, label_test))\n",
    "        accuracyList.append(customAccuracy(predicted, label_test))\n",
    "\n",
    "        clear_session()\n",
    "        del model, time_series_dataframe_test, np_estatico_test, label_test, np_series_cnn_test\n",
    "        gc.collect()\n",
    "\n",
    "    print(accuracyList)\n",
    "    print(sensibilidadeList)\n",
    "    print(especificidadeList)\n",
    "    print(aucList)\n",
    "    return np.array(accuracyList).mean(), np.array(sensibilidadeList).mean(), np.array(especificidadeList).mean(), np.array(aucList).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "sens = []\n",
    "esp = []\n",
    "auc = []\n",
    "\n",
    "figs = []\n",
    "for i in range(n_studies):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "\n",
    "    study.optimize(objective_cnn, n_trials=n_trials)\n",
    "\n",
    "    print(F\"Study: {i}\")\n",
    "    print(F\"Best Value: {study.best_value}\")\n",
    "    print(F\"Best Parameters: {study.best_params}\")\n",
    "\n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    figs.append(fig)\n",
    "\n",
    "    a, b, c, d = evaluation()\n",
    "    acc.append(a)\n",
    "    sens.append(b)\n",
    "    esp.append(c)\n",
    "    auc.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utilsCode.figures_to_html(figs, output_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "    ['', 'média', 'desvio padrão'],\n",
    "    ['Acurácia', round(np.array(acc).mean(),4),  round(np.array(acc).std(),4)],\n",
    "    ['Sensibilidade', round(np.array(sens).mean(),4),  round(np.array(sens).std(),4)],\n",
    "    ['Especificidade', round(np.array(esp).mean(),4),  round(np.array(esp).std(),4)],\n",
    "    ['AUC', round(np.array(auc).mean(),4),  round(np.array(auc).std(),4)]\n",
    "]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "print(sens)\n",
    "print(esp)\n",
    "print(auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}